{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aravind\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BookRecommenderGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(num_features, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.linear = Linear(hidden_channels, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # First Graph Conv layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Second Graph Conv layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Final prediction layer\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "def prepare_data(books_df, interactions_df, min_interactions=10):\n",
    "    \"\"\"\n",
    "    Prepare the Goodreads dataset for GNN training\n",
    "    \n",
    "    Parameters:\n",
    "    books_df: DataFrame with columns ['book_id', 'title', 'authors', 'average_rating', 'ratings_count', ...]\n",
    "    interactions_df: DataFrame with columns ['user_id', 'book_id', 'rating']\n",
    "    min_interactions: Minimum number of interactions for users/books to be included\n",
    "    \"\"\"\n",
    "    # Filter users and books with minimum interactions\n",
    "    user_counts = interactions_df['user_id'].value_counts()\n",
    "    book_counts = interactions_df['book_id'].value_counts()\n",
    "    \n",
    "    valid_users = user_counts[user_counts >= min_interactions].index\n",
    "    valid_books = book_counts[book_counts >= min_interactions].index\n",
    "    \n",
    "    filtered_interactions = interactions_df[\n",
    "        interactions_df['user_id'].isin(valid_users) & \n",
    "        interactions_df['book_id'].isin(valid_books)\n",
    "    ]\n",
    "    \n",
    "    # Create user and book mappings\n",
    "    unique_users = filtered_interactions['user_id'].unique()\n",
    "    unique_books = filtered_interactions['book_id'].unique()\n",
    "    \n",
    "    user_mapping = {uid: idx for idx, uid in enumerate(unique_users)}\n",
    "    book_mapping = {bid: idx + len(user_mapping) for idx, bid in enumerate(unique_books)}\n",
    "    \n",
    "    # Create edge index\n",
    "    user_nodes = [user_mapping[uid] for uid in filtered_interactions['user_id']]\n",
    "    book_nodes = [book_mapping[bid] for bid in filtered_interactions['book_id']]\n",
    "    \n",
    "    edge_index = torch.tensor([\n",
    "        user_nodes + book_nodes,  # Source nodes\n",
    "        book_nodes + user_nodes   # Target nodes\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    # Create node features\n",
    "    num_nodes = len(user_mapping) + len(book_mapping)\n",
    "    num_features = 32  # You can adjust this\n",
    "    \n",
    "    # Initialize random features (in practice, you'd use real features)\n",
    "    node_features = torch.randn(num_nodes, num_features)\n",
    "    \n",
    "    # Create target ratings\n",
    "    edge_weights = torch.tensor(\n",
    "        filtered_interactions['rating'].tolist() * 2,  # Duplicate for bidirectional edges\n",
    "        dtype=torch.float\n",
    "    )\n",
    "    \n",
    "    return Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_weights\n",
    "    ), user_mapping, book_mapping\n",
    "\n",
    "def train_model(model, data, epochs=100):\n",
    "    \"\"\"Train the GNN model\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = F.mse_loss(out[data.edge_index[0]], data.edge_attr)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1:03d}, Loss: {loss:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_recommendations(model, data, user_idx, book_mapping, top_k=5):\n",
    "    \"\"\"Get book recommendations for a user\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings for all nodes\n",
    "        embeddings = model.conv2(\n",
    "            model.conv1(data.x, data.edge_index),\n",
    "            data.edge_index\n",
    "        )\n",
    "        \n",
    "        # Get user embedding\n",
    "        user_embedding = embeddings[user_idx]\n",
    "        \n",
    "        # Get book embeddings\n",
    "        book_indices = torch.tensor(list(book_mapping.values()))\n",
    "        book_embeddings = embeddings[book_indices]\n",
    "        \n",
    "        # Calculate similarity\n",
    "        similarity = F.cosine_similarity(\n",
    "            user_embedding.unsqueeze(0),\n",
    "            book_embeddings\n",
    "        )\n",
    "        \n",
    "        # Get top-k recommendations\n",
    "        top_k_indices = similarity.argsort(descending=True)[:top_k]\n",
    "        \n",
    "        return [\n",
    "            list(book_mapping.keys())[idx]\n",
    "            for idx in top_k_indices\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 010, Loss: 3.2669\n",
      "Epoch 020, Loss: 2.3063\n",
      "Epoch 030, Loss: 2.3651\n",
      "Epoch 040, Loss: 2.2722\n",
      "Epoch 050, Loss: 2.2864\n",
      "\n",
      "Generating recommendations...\n",
      "Top 5 recommendations for user 49:\n",
      "- Book Title 47 (Book ID: 47)\n",
      "- Book Title 81 (Book ID: 81)\n",
      "- Book Title 87 (Book ID: 87)\n",
      "- Book Title 7 (Book ID: 7)\n",
      "- Book Title 48 (Book ID: 48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aravind\\AppData\\Local\\Temp\\ipykernel_13356\\1623634471.py:96: UserWarning: Using a target size (torch.Size([198])) that is different to the input size (torch.Size([198, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(out[data.edge_index[0]], data.edge_attr)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Sample dataset preparation\n",
    "    books_path = \"C:/Users/Aravind/PROJECTS/GNN/data/books.csv\"  # Replace with actual path\n",
    "    interactions_path = \"C:/Users/Aravind/PROJECTS/GNN/data/interactions.csv\"  # Replace with actual path\n",
    "\n",
    "    # Load datasets\n",
    "    books_df = pd.read_csv(books_path)\n",
    "    interactions_df = pd.read_csv(interactions_path)\n",
    "\n",
    "    # Prepare the data for GNN\n",
    "    data, user_mapping, book_mapping = prepare_data(books_df, interactions_df, min_interactions=5)\n",
    "\n",
    "    # Initialize the GNN model\n",
    "    num_features = data.x.size(1)\n",
    "    hidden_channels = 64\n",
    "    model = BookRecommenderGNN(num_features, hidden_channels)\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    trained_model = train_model(model, data, epochs=50)\n",
    "\n",
    "    # Get recommendations for a user\n",
    "    print(\"\\nGenerating recommendations...\")\n",
    "    user_id = list(user_mapping.keys())[0]  # Choose a sample user from the mapping\n",
    "    user_idx = user_mapping[user_id]        # Map to internal user index\n",
    "    recommendations = get_recommendations(trained_model, data, user_idx, book_mapping, top_k=5)\n",
    "\n",
    "    print(f\"Top 5 recommendations for user {user_id}:\")\n",
    "    for book_id in recommendations:\n",
    "        book_title = books_df[books_df['book_id'] == book_id]['title'].values[0]\n",
    "        print(f\"- {book_title} (Book ID: {book_id})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
